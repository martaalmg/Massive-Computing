{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8WG0WLQFgE5"
      },
      "source": [
        "# Participants:\n",
        "Gracia Estr√°n Buyo: 100452014\n",
        "\n",
        "Marta Almagro Fuello: 1004591979\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HGP4k4rPTpNG"
      },
      "outputs": [],
      "source": [
        "#Uncomment next line if you are using Google Colaboratory\n",
        "#!pip install pycuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rHW7jWHuTugs"
      },
      "outputs": [],
      "source": [
        "import  numpy  as  np\n",
        "import  pycuda.autoinit\n",
        "from    pycuda.compiler import SourceModule\n",
        "import  pycuda.driver as  drv\n",
        "import  pycuda.gpuarray as  gpuarray\n",
        "import  pycuda.tools as gtools\n",
        "from numpy import linalg as la\n",
        "from IPython import display\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciWO9Cog66Nz"
      },
      "source": [
        "# Guide to use Tiled Algorithms\n",
        "Whe we wants to use a tiled memory based algorithms, we need to analize the follow steps:\n",
        "1) Tiled Memory Size: What information we will share across all the execution threads in a execution block\n",
        "2) Assign the memory position to each execution thread with memory coalesence\n",
        "3) Fill the Tiled Memory in parallel\n",
        "4) Assign to some threads the extra data needed for the algorithms\n",
        "5) Synchronize the filling memory execution\n",
        "6) Each thread execute his individual task\n",
        "7) Synchronize the execution task\n",
        "8) End the execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDFdRuR-eHCy"
      },
      "source": [
        "# TILED REDUCTION ARRAY\n",
        "The algorithm of reduction (which calculates the sum of all elements in an array), works as follow:\n",
        "\n",
        "![image.png](attachment:d6e8dd16-0624-4d98-af7c-5a8f36bd28c5.png)\n",
        "\n",
        "* Tiled memory size: The tile will contains the twice times the number of threads assigned\n",
        "* Each thread in the execution block copy the data from global memory to the assigned memory place\n",
        "* No extra data need (for this task)\n",
        "* In each iteration\n",
        "    * Adds the possition asigned and the next available data (indexed by the stride)\n",
        "    * This will works until the stride exceeds the block size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ykQwHPFRTw68"
      },
      "outputs": [],
      "source": [
        "tiled_reduction_src = \"\"\"\n",
        "__global__ void tiled_reduction( float *v, float *c, int N){\n",
        "\n",
        "  const int BLOCK_SIZE=1024;\n",
        "\n",
        "  __shared__ float partialSum[ 2 * BLOCK_SIZE ]; //The array dimensions MUST be constants\n",
        "\n",
        "  unsigned int t = threadIdx.x;\n",
        "  unsigned int start = 2 * blockIdx.x * blockDim.x;\n",
        "\n",
        "  \n",
        "  //fill the tile memory\n",
        "  //each thread will fill the memory position start +t and start+blockDim.x+t\n",
        "  //look, each consecutive execution thread (threadIdx.x) will access to coalesced memory in both steps\n",
        "  \n",
        "  if ( (start+t) < N) \n",
        "    partialSum[t]=v[start+t];\n",
        "  else \n",
        "    partialSum[t]=0;\n",
        "\n",
        "  if ((start+blockDim.x+t) < N)\n",
        "    partialSum[blockDim.x+t]=v[start+blockDim.x+t];\n",
        "  else\n",
        "    partialSum[blockDim.x+t]=0;\n",
        "\n",
        "  //Here we will wait until all execution threads fills te memory\n",
        "  __syncthreads();\n",
        "\n",
        "  for ( unsigned int stride=1; stride <=blockDim.x; stride*=2 ) {\n",
        "    __syncthreads();\n",
        "    if ( t % stride == 0)\n",
        "      partialSum[2*t]+=partialSum[2*t+stride];\n",
        "  }\n",
        "  c[blockIdx.x]=partialSum[0];\n",
        "}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyQFaQRC66N3"
      },
      "source": [
        "Here we can not edit automatically the source template to use the string % function to replace character chains in the string variable by other values. \n",
        "\n",
        "The problem to use modern strings formating (format method or f-strings) is the collision of the use of {} symbols, and in the previos source code, the presence of modulus operator (%) collides with the string subtitution.\n",
        "\n",
        "In further codes we will use it to be able to substitute constants values from external variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4fIVY60nWWE9"
      },
      "outputs": [],
      "source": [
        "#Is the BLOCK_SIZE A GOOD BLOCK SIZE????\n",
        "BLOCK_SIZE=1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8ZjAqOa66N4",
        "outputId": "2c6e812b-3dff-4044-f689-15f6dd6d53c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "__global__ void tiled_reduction( float *v, float *c, int N){\n",
            "\n",
            "  const int BLOCK_SIZE=1024;\n",
            "\n",
            "  __shared__ float partialSum[ 2 * BLOCK_SIZE ]; //The array dimensions MUST be constants\n",
            "\n",
            "  unsigned int t = threadIdx.x;\n",
            "  unsigned int start = 2 * blockIdx.x * blockDim.x;\n",
            "\n",
            "  \n",
            "  //fill the tile memory\n",
            "  //each thread will fill the memory position start +t and start+blockDim.x+t\n",
            "  //look, each consecutive execution thread (threadIdx.x) will access to coalesced memory in both steps\n",
            "  \n",
            "  if ( (start+t) < N) \n",
            "    partialSum[t]=v[start+t];\n",
            "  else \n",
            "    partialSum[t]=0;\n",
            "\n",
            "  if ((start+blockDim.x+t) < N)\n",
            "    partialSum[blockDim.x+t]=v[start+blockDim.x+t];\n",
            "  else\n",
            "    partialSum[blockDim.x+t]=0;\n",
            "\n",
            "  //Here we will wait until all execution threads fills te memory\n",
            "  __syncthreads();\n",
            "\n",
            "  for ( unsigned int stride=1; stride <=blockDim.x; stride*=2 ) {\n",
            "    __syncthreads();\n",
            "    if ( t % stride == 0)\n",
            "      partialSum[2*t]+=partialSum[2*t+stride];\n",
            "  }\n",
            "  c[blockIdx.x]=partialSum[0];\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(tiled_reduction_src)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TXTkEKD566N5"
      },
      "outputs": [],
      "source": [
        "mod = SourceModule(tiled_reduction_src)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "A1Jle_oO66N5"
      },
      "outputs": [],
      "source": [
        "datasize=np.int32(1000000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QrRV7yDw66N6"
      },
      "outputs": [],
      "source": [
        "tiled_reduction = mod.get_function(\"tiled_reduction\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oN4nSpbKWdSk"
      },
      "outputs": [],
      "source": [
        "data = np.random.randn(datasize).astype(np.float32)\n",
        "data_gpu=gpuarray.to_gpu(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UasVzUSoYV8E"
      },
      "outputs": [],
      "source": [
        "block_size=(int(BLOCK_SIZE),1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_72v3Ef9btTA"
      },
      "outputs": [],
      "source": [
        "numblocks = int(np.ceil(datasize/BLOCK_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SBkA74pHcala"
      },
      "outputs": [],
      "source": [
        "c_gpu=gpuarray.empty((numblocks,1),np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "p1P4majmb2-c"
      },
      "outputs": [],
      "source": [
        "grid_size=(numblocks,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ke13jd3wb56p"
      },
      "outputs": [],
      "source": [
        "start_t = time.time()\n",
        "tiled_reduction(data_gpu,\n",
        "                c_gpu,\n",
        "                datasize,\n",
        "                grid=grid_size,\n",
        "                block=block_size)\n",
        "end_t=time.time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-v572IHAc7ly"
      },
      "outputs": [],
      "source": [
        "c=c_gpu.get()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEVpW3CadCqI",
        "outputId": "fe016ddb-3296-4dd0-fbec-d9675acd6c0c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1926.5502085274682"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "sum(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzSQdkcgdD3e",
        "outputId": "de13d937-0295-4d18-a9dd-0fd128f43603"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1926.55"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "np.sum(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRqu_6mHfpvc"
      },
      "source": [
        "# 1D Convolution\n",
        "\n",
        "The algorithm to implement will calculate the convolution between 2 arrays.\n",
        "\n",
        "The shortest array, called system mask, system response, represents the exit of a system to an special signal called Dirac's delta (signal of infinite height, but limited area under the curve).\n",
        "\n",
        "The second array (the longest one) is the signal to be shaped by our system.\n",
        "\n",
        "Based on this mathematical operation the filters works.\n",
        "\n",
        "The Image Filter algorithms are 2 dimensional convolutions.<br>\n",
        "\n",
        "![image.png](attachment:17f3b373-5760-4edb-b6b3-a233aef56fe3.png)\n",
        "\n",
        "The problem with the tile algorithms are we need extra data to calculate the the correct convolution (halos)<br>\n",
        "\n",
        "![image.png](attachment:e3bc106b-14eb-4935-8de3-549c7a225578.png)\n",
        "\n",
        "Then, our steps to implement the algorithm will be:\n",
        "\n",
        "* Tiled memory size: The tile will contains not only the block size elements, but also the system mask length - 1, to store the halos. Also, we need to store in memory the shared memory the system mask. \n",
        "* Each thread in the execution block copy the data from global memory to the assigned memory place, and few of them will fill the halos.<br>\n",
        "\n",
        "![image.png](attachment:3a76556c-c652-49c4-821a-c5be890b13aa.png)\n",
        "\n",
        "<br/>\n",
        "\n",
        "![image.png](attachment:83d342d6-57dc-439a-8dc7-190a5dc8a10b.png)\n",
        "<br/>\n",
        "\n",
        "![image.png](attachment:5d4f3aa2-d27b-4639-94fc-e9b3ae706820.png)\n",
        "\n",
        "<br/>\n",
        "* Once filled the assigned memory positions, we have to wait for the other tasks (\\_\\_syncthreads())\n",
        "* Now, we have to calculate the convolution between the system mask and the assigned memory position in the signal vector.\n",
        "\n",
        "You have to implement the algorithms in the follow cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "dgy1NBTzdGRA"
      },
      "outputs": [],
      "source": [
        "convolution_src = \"\"\"\n",
        "__global__ void convolution( float *v, \n",
        "                             float *c, \n",
        "                             float *conv,\n",
        "                             int N,\n",
        "                             int c_size){\n",
        "\n",
        "  __shared__ float tile[1024]; // We saw that with 512 could diverge and with 1024  converges\n",
        "\n",
        "  __shared__ float mask[5]; // five is the length of the mask\n",
        "\n",
        "\n",
        "  // We start by defining the variables\n",
        "  int t = threadIdx.x;\n",
        "  int start = blockIdx.x * blockDim.x;\n",
        "  int halo = 2;\n",
        "\n",
        "  // Fill the tile memory\n",
        "  if ((start+t) < N) {\n",
        "    tile[2+t] = v[start+t];\n",
        "  } else {\n",
        "    tile[2+t] = 0;\n",
        "  }\n",
        "\n",
        "    // To load the left halo \n",
        "  if (t < halo) {\n",
        "    if ((start-halo+t) < 0) {\n",
        "      tile[t] = 0; //ghost cell\n",
        "    } else {\n",
        "      tile[t] = v[start-halo+t];\n",
        "    }\n",
        "  }\n",
        "\n",
        "   // To load the right halo\n",
        "  if ((t+halo) >= 512) {\n",
        "    if ((start+halo+t) >= N) {\n",
        "      tile[t+halo] = 0; // ghost cell\n",
        "    } else {\n",
        "      tile[t+halo] = v[start+halo+t];\n",
        "    }\n",
        "  }\n",
        "  __syncthreads();\n",
        "\n",
        "  //fill the mask\n",
        "  if (t < 5) {\n",
        "    mask[t] = conv[t];\n",
        "  }\n",
        "  __syncthreads();\n",
        "\n",
        "  // multiplicate and sum it (convolution definition)\n",
        "  float accu=0;\n",
        "  for (int i=0; i<5; i++) {\n",
        "    accu += mask[i]*tile[t+i];\n",
        "  }\n",
        "  c[start+t]=accu;\n",
        "\n",
        "}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "LnbOMgKbh-d_"
      },
      "outputs": [],
      "source": [
        "filtermask=np.array([1,1,3,1,1],dtype=np.float32)\n",
        "filtermask_gpu=gpuarray.to_gpu(filtermask)\n",
        "filtermask_size=np.int32(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vuIoKeBagf6t"
      },
      "outputs": [],
      "source": [
        "convolved_gpu=gpuarray.empty((datasize,1),np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ECUSD4sFgXi0"
      },
      "outputs": [],
      "source": [
        "BLOCK_SIZE = 1024 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "N9CcLnVHhehn"
      },
      "outputs": [],
      "source": [
        "mod2 = SourceModule(convolution_src)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ofnlEJM8hOzo"
      },
      "outputs": [],
      "source": [
        "convolution = mod2.get_function(\"convolution\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "VJct3fXm_oXz"
      },
      "outputs": [],
      "source": [
        "block_size=(BLOCK_SIZE,1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "G5BCiVXci9rY"
      },
      "outputs": [],
      "source": [
        "numblocks = int(np.ceil(datasize/BLOCK_SIZE))\n",
        "grid_size=(numblocks,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "EFX4ji9dhG2d"
      },
      "outputs": [],
      "source": [
        "start_t = time.time()\n",
        "convolution(data_gpu,\n",
        "            convolved_gpu,\n",
        "            filtermask_gpu,\n",
        "            datasize,\n",
        "            filtermask_size,\n",
        "            grid=grid_size,\n",
        "            block=block_size)\n",
        "end_t=time.time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoQT98dqjKkP",
        "outputId": "7cdbbb15-ab90-43ae-cf86-8a8d84fc4df8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-4.3711133 , -2.5286596 , -5.394397  , ..., -2.579719  ,\n",
              "       -0.12395483,  1.7146522 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "local_convolved = np.convolve(data,filtermask, mode = 'full')\n",
        "local_convolved[2:-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwMLSL4njKVN",
        "outputId": "19761959-de71-45d0-ecd1-485ca1ed6cea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-4.3711133],\n",
              "       [-2.5286593],\n",
              "       [-5.394397 ],\n",
              "       ...,\n",
              "       [ 1.7146521],\n",
              "       [ 1.1187958],\n",
              "       [ 0.7496468]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "convolved = convolved_gpu.get()\n",
        "convolved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-kCmUa0lVpk"
      },
      "source": [
        "We can observe that the results that we got with the numpy function and the with the tile algorithm is the same. We start noticing the difference in the seventh decimal in most of the cases. "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}